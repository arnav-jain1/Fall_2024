Memory is limited, usage keeps going up
# Background info
CPU can only directly access memory or registers
	Instructions and data must be in memory to be accessed

Memory is just a large array of bytes
Accessing it is slow (compared to register/cache)
	Cache faster because smaller and closer (on CPU)
	Memory takes more clock cycles to access and you have to take a BUS
Memory needs to be protected 


# Old model
OS is generally placed in low memory address (for interrupt vector tables)
![[Pasted image 20241111140957.png]]
Each process has its own address space assigned to it with a base and limit (stored in registers)
If process requests address outside the range, then seg fault


# Address binding
Instructions and data is all binded (stored) in memory addresses 
There are 3 possible times where addresses can be bounded:
	1. Compile time: If the location of the memory is known at compile time, absolute code can be generated (will have to recompile if address changes)
		Can be done when using obj files
		Necessary that the generated address places are placed at the right places
		location should be free when running
	2. load time: Compiler has to generate relocatable binary if the memory location is unknown at compile time (code placable anywhere in memory)
	3. Execution time: Binding delayed until runtime to allow program to move while being ran
		Need hardware support but most flexible 
		Most common way but has overhead
![[Pasted image 20241111141344.png]]

When you compile normally, the instructions are symbolic with no actual values, instead relative values

Logical address space (generated by CPU) is bound to a physical address space
	Changed by the OS
Logical == physical in compile-time and load-time
Different in execution time


## MMU, memory management unit
Hardware that maps a virtual address to a physical address
Simple scheme:
	Start process's logical address at 0
	Max address stored in limit register
	Base memory address stored in relocation register
	The physical address is added to the relocation register 
![[Pasted image 20241111143404.png]]
Range for virtual: 0 to limit register
Range for physical: relocation register to relocation register + limit register
Done in hardware because faster

In compiletime/load time, does not exist 


## Dynamic loading
Static loading:
	entire program is in memory before program starts
	Program size is limited
Dynmaic loading:
	Routine not loaded until called
	Better util, unused routine never loaded (like with if else blocks)
	Useful for edge cases (used rarely but need a lot of code)
	No support from OS, implemented through program design

## Dynamic Linking
Static Linking:
	Links all files at compile time
	Compiles libraries and into a binary and links it as compiles it at compile time
	Increases binary size and multiple copies of the library may exist at a time
Dynamic Linking:
	Linking postponed until exec time
	Library stored in memory, a small piece of code (stub) used to locate the location of the routine (pointer to the function pretty much)
	Only one copy of the library at a time
	*Shared* library

## Swapping
Swapping is when a process is moved from the memory to a *backing store (disk)* and then swapping another process to the freed space
	Pages get swapped in and out
Useful for when memory runs out
Since it is stored on disk and disk is slow, it is very costly
	Leads to memory thrashing if done a lot

![[Pasted image 20241115130500.png]]


# Contiguous memory allocation
Effecient allocation of memory is important because it is limited and contains the OS/active processes
Divided into 2 partitions: The OS in the lower region with intrurrupt vec table and user processes in higher memory

Contiguous memory allocation is when each process is in a single continuous memory block
	Allows for multiple processes to be there at the same time
	Processes protected from each other


## Hardware support for prot
Limit registers used to make sure the address is less than the limit, if it is, then the relocation address is added
	Protects processes from other processes and protects OS
![[Pasted image 20241115131425.png]]
Since relocation register is the lower bound, we dont have to check the lower bound


Multiple parition method:
	Holes are blocks of available memory and are scattered through address space
	When a process arrives, it is allocated memory in the hole
	The OS remembers (saves) info about the hole and allocated partitions
	![[Pasted image 20241115131637.png]]

The issue with this, is how does the OS allocate memory for a new process of size n
	**First-fit scheme**: Allocate first hole that is big enough, fastest
	**Best-fit**: Allocate the smallest hole that is big enough (searches the list unless ordered), produces smallest leftover hole
	**Worst-fit**: Same thing but for biggest hole. Produces biggest leftover hole (not effecient)



### Fragmentation
External Fragmentation: There is enough memory to run the process but it isn't continuous
Solutions:
	Compaction: Move blocks to place the holes together, only possible if dynamic
	Allow for non continuous allocation of address space
Internal Fragmentation: 
	Happens with blocks of memory where the size of the block is slightly larger than needed so that the hole isn't super small
	Leads to memory loss

# Paging
Solution to external fragmenting and compaction
Commonly used
Depend on HW support
Allows for noncontinuous address space blocks

Scheme:
	Divide the *physical memory* into fixed size blocks (frames), usually power of 2
		Refer to the actual address
	Divide the *logical address* into blocks of the same size (pages)
	OS tracks all of the frames
	A program that is **n** pages big, needs n free frames
	Page table translates pages into frames
Each of the pages matches up with a frame in the phsyical address space

The size of the page == size of frame (specified by the hardware)
The page size determines the amount of bits needed to specify the offset

The offset is the number of data a page can hold

So if our page size is n bytes, the offset will be $log_{2} n$  

So lets say the page size is 32, the offset will be 5 
Now lets say the total address bits is 12 (arb)
then the page number is now 12 - 5 (offset) = 7 so then there are 2^7 pages

Lets instead say address bits is 7, so then 7-5=2 so 4 pages
![[Pasted image 20241115142238.png]]

![[Pasted image 20241115142721.png]]
So lets say finding logical address 1 to physical
The page # is 00 (binary) and offset is 01 so the physical would be 10101=21

Lets look at 5, it is on page 1 so 01 and offset is 01 so 0101. The physical would be 6 (page table) so 110 and offset stays same so 01 so 11001 which will become 25

For 10, 1010 (write in binary is step 1) 10 regers to page 2 which refers to page 1 so 001 and then the offset carries over which is 10 so then it becomes 00110 which is 6

For 15, 1111 so offset is 11 and page num is 11 (3) which corresponds to 2 which is 10. Then bring the offset back so 01011 which is 11


### issues with page tables
This still leaves internal fragmentation 
Page size
	Small page size reduces internal fragmentation but increases size of page table
	Larger page size reduces size of page table
	Larger transfers is more effecient 
Each memory load/store operation goes through the page table
	Page table has to be fast (hardware registers)
	Only possible for small tables, 256 entries max
Each process can have its own page table
	Page table context switching should be fast
Most computers have large page tables with pointers to the page table (in memory)


### Implementation
Page table base register (PTBR): Points to page table 
	Only part affected with a context switch
Every load/store requires 2 memory accesses
	One for the page table and another for data/instruction
Translation lookaside buffer (TLB):
	Cache to hold popular page table entries
	Should be fast, uses associative cache
![[Pasted image 20241115144819.png]]
Power hungry part but very important

#### Issues
Small page size reduces internal fragmentation (less wasted space) but increases page table
Large size does the opposite

Each memory and stores goes through the page table so it has to be fast (registers) which means limited to 256 entries
Each process has its own page table
Most Computers allow large page tables (kept in memory) where register points to page table address



Page-table Base register (PTBR) points to page table (only this register is affected when context switching)
	Every load/store uses 2 memory access (1 for the page table and another for the actual data)
Translation look-aside buffer (TLB) Cache that holds the most popular page table entries
	Needs to be very fast (associative cache)
	Since it an associative cache, it can search for everything at the same time (takes power)
		But if it is too large then it takes too long and more power

### Memory protection
<mark style="background: #FF5582A6;">If CPU has 4 address bits, then the logical address space is 16 bytes, page size is 4 bytes</mark>
	If there is are 2 processes P1 and P2 that are using 16 and 6 bytes respectively, both will have a page table that is 4 pages long
	The unused one are set to invalid

Associate protection bit to each frame
	Valid is to show the address space maps to some frame which is good and allowed
	Invalid shows that there is nothing is there and that it is not mapped to some frame (which is why you get a seg fault)

Another example:
	14 bit addy space where the program goes from 0-10468
	Page size of 2kb 
		So 8 pages total
	Pages 6 and 7 exist but are invalid
	![[Pasted image 20241120143646.png]]

### Shared pages
Allows the sharing of code
	The code is set to read only and is shared
	Each process has its own area as usual
	reduces memory redundency
Example
	![[Pasted image 20241120143825.png]]
	Frames 3 4 and 6 are shared 
	All still have their own data though
How dynamically linked libraries work

Sometimes inadequate because the page table can become huge
Limit used to be 4 gb because 2^32 (32 bit machines) is 4 gb 

# Complex page tables
## Hierarchical page tables
Page tables divided into pieces (paged page table)
Done in order to handle larger page tables (32 bit to 64 bit)
Pages divided into 2 parts, p1 and p2
![[Pasted image 20241120144833.png]]
	p1 is an outer page table which points to their own page tables
	2^10 page tables 
	p2 points to the correct page table 
	Offset normal 
	2^32 points to 2^32 which has 12 bit offset
![[Pasted image 20241120145009.png]]
Its literally the same otherwise

Two level is not big enough for 64 bit
	We can increase to 3 level but that sucks because it is more memory accesses (slow)
	n levels require n+1 accesses

## Hashed page table
Used for >32 bit address spaces
Each entry has a linked list of elements with
	virtual page number
	page frame num
	next pointer
Algorithm
	Hash the page number into hash table
	Compare each element with each elem in the linked list until found 
	Form the physical address from the page frame
![[Pasted image 20241206133841.png]]


## Inverted page table
Tries to overcome drawbacks of big virtual address spaces like huge page tables and lots of memory

Aproach:
	Table has one entry for each physical page
	each entry has virtual page number for that frame (frame i has page number associated with it)
	![[Pasted image 20241206134442.png]]
	The process ID maps to a page and the index that the process id is at is the frame number

Lowers memory (does not store each page table) but uses O(n) search to find the pid, more there are the more time it takes
	Can use hashtable

# Segmentation
A way to manage memory that mimics the user view
	stack, heap, data, text all seperate "segments" 
![[Pasted image 20241206140210.png]]
Logical address is comprised of <segment num, offset>
Segment table maps logical addresses to physical addresses
	Base: Starting physical address where segments start in memory
	Limit: Length of the segment

Segment table base register (STBR): points to where the segment table is in memory
Segment table length register (STLR): number of segments being used by a program
	IF segment number >= s, then illegal segment

![[Pasted image 20241206140134.png]]

# Example Intel pentium
Uses both segmentation and segmentation with paging
CPU gen logical address -> segmentation unit -> linear address -> paging unit -> physical address

32-bit architecture 
Max number of segments is 16k
	8k in local descriptor table
	8k in global descriptor table

Max size of segment is 4gb ($2^{32}$ bytes)
Each segment descriptor is 8 bytes (has base and limit)
Each logical address is 48bit (16 to select segment, 32 offset)
	Selector has 13 bits of segment number, 1 bit table identifier, and 2 for perms
	48bit linear becomes 32bit logical
	![[Pasted image 20241206141153.png]]

Page size size is 4kb or 4mb
4kb:
	2 level paging scheme, p1 and p2 are 10 bits, offset is 12 bits
	Base of page directory pointed by CR3, accessed by first 10 bits
	Page directory gives base of inner page table, accessed by second 10 bits
	Final 12 bits is the offset
![[Pasted image 20241206141502.png]]

1. Find the location of the page that you want on the disk
2. If there is a free frame, use it. Otherwise use an algorithm to find a frame to replace (called the victim frame)
3. Bring the page into the frame that was freed up
4. Restart the process
To reduce the page fault overhead, use a modify bit that will only copy the page to memory if modified
![[Pasted image 20241208181905.png]]

# Page replacement algorithm
Goal: get the lowest page fault rate
Schemes:
	FIFO 
	Optimal page replacement
	Least recently used
	LRU approx
	Counting based replacement

To evaluate, we can simulate replacement on a string of memory page references and calculate the page faults

Expected graph ![[Pasted image 20241208182206.png]]
**Belady's anomaly**: Page fault rate sometimes increases when number of frames increases
## FIFO
Simple algorithm
	Each page has a time it was brought in, the one brought in first (oldest) is the one that gets removed first
	Replaced at head of queue, new page at tail of queue

Prone to belady's anomaly
![[Pasted image 20241208182606.png]]

## Optimal page replacement
Algorithm with the lowest fault rate
	Replace the page that won't be used for the longest amount of time
	No Belady's anomaly
	Provably optimal but needs future info
![[Pasted image 20241208183252.png]]
## Least Recently Used (LRU)
Approximates the optimal page replacement algorithm by keeping track of when each page is used and replacing the one that hasn't been used for the longest time
	Very good policy
	Requires hardware support in order to be fast and accurate
	No Belady anomaly	
![[Pasted image 20241208183417.png]]	
Multiple ways to implement

Counter Implementation
	Every page table entry has a field for *time of use*
	Copy the clock into the time of use field on every access
	Then when replacing, find the entry with the smallest value
	Requires full search and a memory write every time accessed
	COunter could overflow or take a lot of space

Stack implementation:
	Keep a stack of the page numbers in a linkedlist
	Move page to the top when it is referenced
	Remove bottom of stack
	No search required

Reference bit implementation:
	Associate reference bit with each page
	When referenced, set the bit. Clear it periodically
	Replace the one that has bit = 0 (if exists)
	Can't store the order of accesses

Record ref bit algorithm:
	Each page has ref bit and 8 additional bits
	Periodically, shift the 9 additional bits right and set ref bit as 0
	![[Pasted image 20241208184059.png]]

Second chance algorithm:
	Single ref bit as well as a basic FIFO (actually a circular LL but clsoe enough)
	The pointer goes through the FIFO looking for a ref bit 0, if there is one then the victim frame is found. Otherwise, if it is 1, then it is set to 0 and given "a second chance"
	If all are set to 1, then the first one checked would have been set to 0 first thus be used
	![[Pasted image 20241208184511.png]]

## Counting based algorithms
Counter of how many times a page is referenced
Least freq used: Replace the page with the smallest count
Most freq used: Replace the page with the largest count
Neither are very good

## Page buffering algorithms
Optimizes by Maintains list of victim frame
There is a buffer for the victim frames so that the new page is loaded immediately (both are in memory at the same time)
Scheme 1:
	Essentially, start running the program while the replaced page is being written to memory
	Don't need to wait until the replaced page is saved (async)
	(The victim page is copy-out)
	Page A gets buffered and then written into disk while page B is running
Scheme 2:
	The victim pages buffered until idle and that's when they are written
	(copy out when idle)
Scheme 3:
	Remember which page is in each victim frame (buffer them)
	If process wants a page that was a victim frame, it can just fetch it (useful for when a victim frame is used right after becoming the victim)

## Allocation of frames
How many frames does the OS need to allocate? Min? Max?
Min:
	Min depends on the max number of pages any instruction in the ISA can reference at once
		For direct addressing, this is 2 (one for the instruction, another for data)
		For indirect, 3 (instruction, address of data, data)
		Others might need more
	If min not provided, then it can't run properly
No real max, but don't want to give too much because it will slow down others

### Algorithm
Equal allocations:
	If there are *n* processes and *m* frames each process gets m/n frames
Proportional allocations:
	Get total number of frames (sum all frames), divide the process frame req with the total then mult by total number of frames
	![[Pasted image 20241209124247.png]]

Global replacement: Process selects one frame to replace from ALL frames 
	Process can take from another process
	Can't control its own page fault rate 
Local replacement: Selects process from its own frames to replace
	May be slower, but can control page fault rate

Non-uniform memory access:
	Processor has memory that is "closer" (not shared) which is lower latency and faster

## Thrashing
Process spends more time paging than executing
	Process keeps swapping pages in and out, high page fault rate
Usually because there aren't enough frames allocated to it

Cycle:
	Not having enough frames causes page faults
	This lowers CPU util
	OS will think it needs more compute so another process is added
	Even less frames for the processes so the cycle continues

Can prevent by reducing the number of active processes (so the process gets more frames)
Or by just allocating more frames

Multiprogramming leads to more thrashing
![[Pasted image 20241209125432.png]]

### Prevention

Prevent by just giving more frames (increase physical memory)
Working-set model:
	Estimates how much memory a process needs
	Based on locality model: Each process uses a small set of memory ref/frames, exec moves from one phase to another
		During one phase, the program is doing something similar
	The number of frames for each set is the *working-set*
Implementation:
	Assume a working set window $\Delta$ (moving window that can be 5ms for example, number of distinct pages used for each window is WSSi)
	WSSi (Working set size of process Pi) = total pages referenced in most recent $\Delta$
	Total demand is the sum of WSS(i)
	If D > m (frames) then thrashing is likely, suspend a process
	Working set has to be estimated

![[Pasted image 20241209141009.png]]
The page fault rate will spike at the start of the working set then fall until the next


Page-Fault Frequency scheme:
	Working set is complex and makes assumption
	Contrary to the previous cycle, instead of using CPU util for deciding more processes, use page fault rate (PFR)
	If PFR is too high, then lower the processes and/or increase the frames
	If PFR is too low, then increase the processes and/or decrease the frames
	![[Pasted image 20241209141952.png]]

# Memory mapped files
Rather than using syscalls to read/write to file, we map the disk to memory 
	Removes the syscalls
	Converts disk to memory access (faster)
	Simplifies disk

Mechanism:
	File read first using demand paging
	Page size chunk of file is read from file system into a physical frame
	After that the read/writes are just ordinary memory accesses

This allows several processes to map to the same file so the pages are shared
	![[Pasted image 20241209142730.png]]
	This is the method to share memory in some OS

There may be special IO instructions to transfer and control data to the IO controller (CISC)
Memory Mapped IO (RISC)
	IO device registers mapped to logical address spaces 
	Fast and good
		Fewer instructions (less IO complexity)
	Worse because you give up logical address space to IO devices
	Address spaces where you read/write that will go to IO device
May be control bit to know if data is available or not
	If CPU polls the control bit (checks regularly) then programmed IO
	If device sends interrupt then interrupt driven IO


# Allocating Kernel memory
Kernel memory is allocated from free memory pool
Does not use paging
	Some memory needs to be continuous 
	Want to minimize waste due to internal fragmentation
	Kernel requests memory for structures of very different sizes
Strats for managing memory (having contiguous without wasting)
	Buddy system
	Slab allocator

## Buddy system
Satisfies requests in units that are power of 2
Request rounded up to the next highest power of 2	
When smaller chunk is available, then split the chunk into two buddies of same size (div by 2)
	Continue until splitting would make it too small
21kb requested from 256kb
![[Pasted image 20241209144155.png]]
11kb of internal fragmentation

When done, the blocks will be combined


## Slab Allocator
Slab: Several physically contiguous pages
Cache has >=1 slabs
Single cache for each unique kernel DS
	Cache filled with instantiations (objects) of the DS
Slab allocation algorithm:
	Create cache of obj in cont space (mark as free)
	Store objects in free slots (mark as used)
	If the current slab is full, then allocate the next object from empty slab
	If no empty slab, go to free slots in next cache
Pros:
	No fragmentation
	Fast memory
![[Pasted image 20241209144932.png]]



