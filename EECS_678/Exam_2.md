# Ch 4
## Thread termination
Killing a thread before it is done
Asynchronous cancellation:
	Cancels the target thread immediately 
	Allocated resources may or may not be freed
	Shared data may be ill-defined
Deffered cancellation: 
	Target thread elims itself
	Easy to make orderly
	Failing to check cancellation status may cause issues
Linux supports both (pthread_cancel)

## Signal handeling
signals used to notify a process about an event
Signal handler processes these signals
	OS can either handle it or deliver it to the proper process
Types:
	Asynchronous: Generated by an event outside the process
	Synchronous: Generated by event inside the process
Options on where to deliver
	Deliver to thread where signal applies
	Deliver to every thread
	Deliver to certain thread
	Assign a specific thread for all signals

## Implicit threading
Writing correct multithreading is hard and can slow down performance/latency
Compilers and runtime libraries can manage threads semiautomatically
	Thread pools
	OpenMP

## Thread pools
Fixes the issue of continously creating and destroying threads which is slow and/or trying to create a thread you have no more space/processing power
Thread pools create a list of threads in a pool where they wait for work, at most the number of processors
This makes it faster to service a request because you don't have to make a new thread every time and makes it so you can't exceed a certain number of threads

## OpenMP
Compiler directives and API for C, C++, Fortran
Supports parallel programming 
User defines a parallel region using scope and OMP creates as many threads as possible

## Kernel threads linux
Linux refers to them as tasks
Creation done through clone() syscall and allows child to share address space of parent task (process)
![[Pasted image 20241011132212.png]]

# Multicore processors
Multiple processing cores on a chip
Speed is reaching a limit because of power/heat, limits to freq scaling, transistor scaling is still viable
	Transistors are leaky (lose power) which is why they need so much power which generates heat
More cores on one computer > more computers because same chip communication is faster and memory sharing is easier and faster
![[Pasted image 20241011132430.png]]
Challenges:
	Dividing activities and balancing workload
	Splitting data
	Data dependencies between currently running tasks
	TEsting and debugging

# Ch 5 sync
## Why important
One issue with parallel programs are race conditions
2 major issues
	Memory problems
	Race conditions

Why synchronization?
	Processors cooperate with each other 
		Producer and consumer model
		Concurrent execution (multicore)
	Processes share data
		When we share data, it may stop being correct
	Why is correctness in danger?
		Context switches can happen at any given time (interleaving, leaving mid process)
		Processes run concurrently
		Different orders of updating the data may lead to different values (not functional)
	Process synchronization
		Done to coordinate updates to shared data (multiple processes writing at the data at the same time)
		No issue if there are no writers

![[Pasted image 20241011141516.png]]
Bounded buffer problem: Writes cannot happen if buffer is full
2 counters:
	*In* is the index where we put in the data and out is where we pull *out* the data. 
	Counter has number of items in the buffer, shared
If counter++ is atomic then there is no issue but if it is not then 
If there are multiple producers then in is also critical
![[Pasted image 20241011142306.png]]
Atomic instruction is the most basic unit that WILL complete and can't be interjected. So R1 = load(counter) WILL happen but you can interject between instructions
If counter++ is atomic (a single instruction) then this issue will not happen

Race conditions:
	Several processes manipulate (write, etc) data at the same time and final value depends on the order
	*Race* between multiple processes

The critical region that updates shared data
Updating data at the same time is dangerous
## Solution
Simple Solution (in theory) is to only allow **ONE** process to enter and be in the critical section at a time
	Hard in practice
	Protocol:
		Request permission to enter critical section
		Indicate exit from section
		Only one process at a time
Solution should include:
	Mutual execlusion: Only one process at a time
	Process: Selection to enter should be fair and decision not postponed forever
	Bounded waiting: Fixed amount for how long the wait will be before access to critical section (timed and/or line ie at MOST 5 min or at MOST 3rd in line)
	Also guarding against *deadlocks*

# Small aside, Preemptive vs Nonpreemptive kernel
OS has many processes and user is using many parts of the OS that is shared (like the file and memory management table)
premption: Process time slice expires so going back to ready

Preemptive kernel: Process can be preempted (moved back to ready when time is up) while executing kernel mode program
	Can run into issues when it is moved to ready before done with (shared data)
		IE updating file in file table, if moved before done, it is a shared data so it might not be what you expect
		Good for real-time programming because more responsive
	Linux
Non-preemptive kernel: OS code can run indefintely 
	Interrupts disabled 
	Became less popular with multicore processors because harder to do and not effecient 


# Sols to Critical section problem
## Peterson's sol
Software based
Supports only 2 processes (can be extended but looks ugly)
2 shared vars:
	Turn: Whose turn it is to enter the critical section
	flag: indicates whether it is ready to enter 
![[Pasted image 20241011144617.png]]
Flag indicates whether it is ready to enter the section
The while loop blocks
Not guarenteed which one goes first

Meets all requirements:
	P0 and p1 never together at the same time
	P1 does not have to wait if p0 does not want to enter
	process waits at most one turn
But only 2 processes (others are very messy and bad)
Assumes load and store are atomic
Also assume that the memory accesses that are unordered
Might be less effecient than hardware approaches (especially if >2 processes)
Another issue is that one process can become blocked (execute while loop while not doing anything)

```
P_0: flag = [1,0]
P_0: turn = 1
P_0: while (false && true)
P_0: Critical section

P_0: flag[1,0]
P_1: flag[1,1]
P_0: turn = 1
P_1: turn = 0
```

## Locks
General solution
	Critical sections are locked 
	Processes lock on entry and then unlock on exit
## Hardware support
For 1 core:
	Concurrent processes cannot overlap, only interleave
	Process runs until sys call/interrupt
To fix an interrupt happening while in critical section just disable and reenable but it disabling it can create infinite loops

Multiprocessors: Processors are doing stuff independanly 

Disassabling interrupts is too ineffecient and not scalable for OS
Hardware support with atomic instructions
	Atomic test and set, swap, compare and swap
	Treated as one step, cannot be interleaved
![[Pasted image 20241018143842.png]]
target is saved, then changed to true, then the old target is returned
all in one step

![[Pasted image 20241018144047.png]]
If mutex is false, it is set to true and will continue through the while loop since it returns false. 
If it is true, then it will continue

3 desirables:
	Mutual exclusion: maintained because only at once
	Progress is mantained because process gets access right away if critical section is free
	Bounded waiting (fairness): Not guarenteed because one can be in critical section for any amount of time



Getting fairness
![[Pasted image 20241021141241.png]]
Assumption: 
	processes numbered 0..n
	waiting is shared but one spot for each process
Initially, the lock is false

the while loops keep going until unlocked
The j part checks (sequencially) if the other processes are waiting and then it will hand the lock over to another process (only unlocks if none are waiting)
This gives fairness
<mark style="background: #FF5582A6;">not entirely sure why</mark>


## Semaphores
Higher level solution than ISA instructions (provided by OS)
Similar to locks but with different semantics
Simple def:
	int value accessed in init, wait, and signal
	All are atomic
Binary semaphores: 0,1 for mutex
Counting semaphore: any int value with access to some finite value

![[Pasted image 20241021142700.png]]
***SIGNAL INC, WAIT DEC***
Similar to before
S should always be 1 or 0
Only one process waiting at a time, it will wait until the other process gets to the signal

Desirables
	Mutual exclusion: maintained because only at once
	Progress is mantained because process gets access right away if critical section is free
	Bounded waiting (fairness): Not guarenteed because one can be in critical section for any amount of time

Spinlocks: Process spins forever
Issues with the previous ones is that the processors are waiting and not doing work, they are spinning and the wait could be forever
	Note: Multiprocessors still use busy waiting


### Semaphore w no busy waiting !!
Associate waiting queue with each semaphore 
```c
typedef struct {
	int value;
	struct process *list
} semaphore;
```
![[Pasted image 20241023141941.png]]
wait will suspend the process entirely and signal will bring it from ready to execution
These 2 still need atomicity
	One way is to dissable interrupts
	Another is to have spinlocks around wait and signal
Spinning not entirely gone but shifted (and reduced)


### Deadlock/Starvation
2+ processes waiting for each other to do something that only the other can do
Essentially both are just waiting for each other
![[Pasted image 20241023142738.png]]
If both S and Q are 1, then P0 is waiting for P1 (Q to become 0) to finish and and p1 is waiting for p0 (S to become 0) to finish.


Starvation is when process is waiting indefinitely (will never be removed from semaphore queue)
	caused by scheduling
Priority inversion: Lower-priority process holds blocks a higher priority process from running
	Example, 3 processes L < M < H
	Let L be producer and H be consumer
	Since M has higher priority, M preempts L this causes L to be after M and since H needs to wait for L, H has to wait for L and M
	sols: 2 priorities max, give the producer that is lower priority higher priority

#### Bounded buffer problem
Set of resource buffers shared by producer and consumer
Producer waits when buffer full and consumer waits when empty
We want consumer to see each item EXACTLY one time
SOlution (3 semaphores):
	Mutex semaphore that allows exclusive access
	Empty (counting semaphore) which counts empty buffers (initialize to num buff)
	Full: same as empty but for full (initialize to 0)

![[Pasted image 20241023144146.png]]

#### Readers - Writers problem
Some threads only read (not consumers, only read dont remove), others only write
Readers can go at same time but writers can't go concurrently
Want to make sure they don't overwrite each other and readers dont read while being written
2 semaphores:
	mutex: ensure mutual exclusion for readcount, initialized as 1 (readers block each other from updating a counter)
	wrt: ensure mutual exclusion for writers and between writers and readers

Signal always increments semaphore value
## Drawbacks
Essentially just shared global vars
Very low level constructs, the connection between the semaphore and the data controlled by semaphores is non existent 
Hard to use it 

# Monitor
Programming language construct that controls access to data
	Sychronization code added by compiler but enforced at runtime
Abstract data type that has:
	Shared data structure
	code that operate on shared data struct
	Sync between concurrent code
Guarentees that only the monitor code can update the data


![[Pasted image 20241025140953.png]]
Only one thread can execute at a time
Other threads wait
When one active thread exits, another enters

For example in Java, the synchronized keyword indicates that only one thread can access it at a time

The issue is when having to wait
If consumer has to wait for producer and gets there first, then it is bad
![[Pasted image 20241025142253.png]]
To fix this then there is condition values that when the condition is met, the lock is reaquired and the critical section is reentered. 

Condition vars have 2 vars:
	wait(), if cond not met, thread moved to wait set, gives up lock
	signal(), wakesup waiting thread (1), if no process suspended, then does nothing
		Some programs have broadcast to send signal to all threads
Another problem is that if there is an active thread that invokes a signal, what happens to the red suspended thread? It can't go in because it is locked already
2 ways to handle:
	Hoare monitors: signal switches from caller to waiting thread, waiter will have its condition met when it starts because the signal immediately swaps
	Mesa monitors: waiter set to ready, signaler continues and the waiter is sent to the ready queue. Issue is condition might not be met when waiter starts
Signaler should immediately leave and the waiter should start

![[Pasted image 20241025143005.png]]
Producer will add to buffer and then send the signal, if it is full then it waits
Consumer will get the resource and then send the signal back, if it is empty then wait

The conditions are not booleans, they are the signals
Conditions $\ne$ semaphore
	In semaphore wait blocks and signals increases global var count


Sleep vs spin (wait)
Spin: 
	Occupies CPU, slows other threads
Sleep:
	issue wait and sleep, signals to sleeping thread to start and then wakeuping up thread involves lots of context switches

Spin can be faster if critical section is small
Spin waiting used on multiprocess systems, small critical sections, thread holding the lock is running

# 0 is lock

# Basic concepts
Multiprogramming
	Alternate between CPU and IO bursts
		IO burst: Process does IO and not use CPU (free/idle)
		CPU burst: Process does CPU and not use IO
	Can schedule another process during IO burst (max the util)
CPU bound
	Speed bounded to CPU speed (better cpu = faster)
	Most time is doing CPU, long CPU bursts
IO bound
	Spend most time doing IO
	Does not depend on CPU speed
	Few short CPU bursts

# Scheduler
Picks the next process to run
	Part of the OS *dispatcher*
	Selects processes from memory that are ready to execute
	Utilizes a strategy
Happens when process switches from
	1. running to waiting (when IO)
	2. running to ready (time slice is up)
	3. waiting/new to ready (check if the priority is higher than currently running)
	4. terminates
All 1 and 4 is nonpreemptive (process voluntarily finishes), others are preemptive (Involuntarily)

Nonpremptive:
	Process voluntarily is done with CPU
	Easy, no special hardware (no interrupts, timers)
	Bad response time for interactive and real-time systems
Prememptive (better):
	OS forces process to leave  the CPU (higher priority or time slice runs out)
	Special hardware like timer
	Needs to maintain consistency 
	Complicated but preferred
	Favored by OS


## Dispatcher
Scheduler is part of dispatcher
Goals:
	Get new process from scheduelr
	Context switch (remove current process)
	Give CPU new process
	Jump to right spot in new program to restart 
Time taken to do this is called dispatch latency


## Scheduling queues
Job queue: All processes 
	Long term scheduler pareses them and brings them into memory 
	All processes are in job queue
Ready queue: Processes in memory
	Ready and waiting for execution
	Scheduled by short-term scheduler
Device/IO queue: waits for a device
	Process can be blocked for same device 
	IO completion moves back to ready queue

![[Pasted image 20241028141703.png]]
Ready queue is the queue that is waiting to be processed
Can escape the process if:
	IO request, then goes to IO queue and once doen then goes to ready
	Time slice: Put back in ready
	Fork: When child done, goes back to ready
	Interrupt wait: When interrupt occurs, goes back to ready

## Metrics
How to measure how good each scheduling algo is 
Maximize:
	CPU Util: % of time CPU is busy (includes busy waiting)
	Throughput: Number of processes that finish per time unit
Minimize:
	Turn around time: How long it takes to finish a process once submitted (finish-submit)
	Waiting time: How long a process is waiting in the ready queue (time it is not running and not done)
	Response time: Amount of time it takes from request submitted until resposne
Also want to be fair to all processes and users


Evaluation criteria:
	Give importance (or weight) for each metric
Deterministic modeling (what we use):
	Take a workload (group of processes) and gets the perf results for each algorithm
	Simple and fast w exact numbers
	Difficult to generalize (what tradeoffs are good/bad may vary)
	Shows algo trends

Workload model:
	![[Pasted image 20241028142738.png]]
	Shows list of processes and information
	Process 1 arrived at 0 and did a CPU burst for 8s
	P2 arrived at 1 and burst for 4
	P3 arrive at 1 (but exec after because P2 is first) and burst 10
	P4 arrived at 6 and burst for 2

Gantt chart:
	For batch scheduling algorithm (non premp)
	![[Pasted image 20241028143022.png]]
	P1 starts, exec for 8 then P2. P2 exec for 4 then P3 for 10 then P4 for 2

Same workload model will have different gantt charts

![[Pasted image 20241028143342.png]]
For process A:
	Submitted at 0
	Response time is 0
	Turnaround time = 9
	Wait time is 2+2+1 = 5 (time it is not running and not done)
For process B:
	Submitted at time 0
	Response time is 1
	Turnaround time (submission to completion) = 5
	Wait time = 3


Queuing model:
	Theoretical model
	Represented by a bunch of equations 
	Depends on a bunch of assumptions
Simulations:
	Most common, best
	Simulate a coded scheduling algo based on a bunch of data
	Time and space intensive
	Not always deterministic (small variations, systems are complex)
	Overhead of algorthim (how long it takes to decide) is considered

![[Pasted image 20241028143840.png]]
Get trace of a real system (list of processes ran inlcuding cpu burst, times, etc) and then run the trace


# Algos
## First come first served
The first request is the first served (FIFO)
No preemption 
	No time slice
Advantages:
	Easy to write
Disadvantages:
	waiting time might be long
	Doesn't balance IO bound and CPU bound processes
	Convoy effect: Long process before short process so the average wait time goes up
	Not usable for time sharing systems
![[Pasted image 20241030140814.png]]
P1 will run from 0 to 24, then P2 from 24 to 27 and then p3 from 27 to 30
Waiting time $WT(P_{1})$ for $P_{1}$ = completion time - submission time - time burst (running time)
	= 24 - 0 - 24 = 0
$WT(P_{2})$ = 27 - 0 - 3 = 24
 $WT(P_{3})$= 30 - 0 - 3 = 27
Average wait time = (0+24 + 27)/3 = 17

Turnaround time for $P_{1}$ = Completion time - submission time
	= 24 - 0 = 24
$TT(P_{2}) = 27 - 0 = 27$
$TT(P_{3}) = 30 - 0 = 30$
$Av(TT) = \frac{24+27+30}{3}=27$



If the order is $P_{2}, P_{3}, P_{1}$
This improves everything significantly
![[Pasted image 20241030141718.png]]
Issue is that the average is very variational, and no preemption (bad for interaction and real time systems)
But the scheduling is constant time, it is fast and scheduling the job is fast and always the same

## Shortest job First (SJF)
Order each of the processes based on how long their burst is (how long it takes to run)
The shortest job runs first
Advantages
	SFJ is a greedy algorithm and optimal because the wait time will always be the shortest possible 
	Good benchmark
Disadvantages:
	Difficult to know length of the next CPU request (user might not even know) so it is difficult to implement (unrealisitc)
	Leads to process starvation if there is a process with a long CPU burst and there are always shorter processes 

![[Pasted image 20241030142407.png]]
P4 (3) runs from 0 to 3 -> P1 (6) runs from 3 to 9 -> P3 (7) runs from 9 to 16 -> P2  (8) runs from 16 -> 28

$WT(P_{3}) =$ 16 - 0 - 7 = 9
Av wait time = 7
We know that 7 is the BEST wait time

Since we don't know how long it will take, we need a way to predict it
### Estimating length of next CPU burst
Done by looking at CPU burst of how long the process took in the path
	Calc as exponential av
$t_{n}$ = actual length of nth CPU burst
$\tau_{n+1}$  = predicted value of the next burst, random at the start for n=1
$\alpha, 0 \le \alpha \le 1$ 
$\tau_{n+1}= \alpha t_{n} + (1-\alpha)_{\tau_{n}}$     
If $\alpha = 0$, then history does not count, random guess
If $\alpha=1$ then only the last CPU burst counts

$\tau_{n+1} = \alpha t_{n} + (1-\alpha)(\alpha t_{n-1} + (1-\alpha)(...))$   so the previous generations are weighted less and less because the $\alpha$ gets squared for the gen before the prev, cubed for the one behind
![[Pasted image 20241030143957.png]]
Inital guess was 10, actual was 6
so $\tau_{2} =.5*6 + .5 * 10=3+5 = 8$
Actual $t_{2}=4$ 
so $\tau_{3}= .5 * 4 + .5 * 8 = 6$
...

One of many models


Adding preemption to SJF is when a new shorter process comes in

### Preemptive SJF
Previous one is not preemptive, once scheduled, it will run for full time slice
With this one, if a new shorter process is scheduled, it might preempt 
For review, prememptive if:
	New process created
	Time slice expired
	IO done 
	Higher priority process

$WT(P_{1})$ = 12 - 0 - 8 = 4
$WT(P_{2}) =$ 5 -1 - 4 = 0
$WT(P_{3})$ = 26 - 2 - 9 = 15
$WT(P_{4})$ = 10 - 3 - 5 = 2
$WT(Av)$ = 6.5


### Priority Scheduling
Every process has a priority and CPU does highest priority 
Externally assigned (someone else specifies, like I specify when I put it in)
Internally assigned (intrinsic to the algo like SJF and FIFO)
Preemptive or non preemptive 

Lower number = higher priority 

Advantages: Priorities made as general as needed
Disadvantage: Lower priority might never exec
Aging: Used to prevent starvation, Increase the priority of the process with time

### Round Robin scheduling
Each process given a fixed time quantum and then preempted and then next process runs
	Ran in FCFS 
	Allocate CPU to first job in queue until its time slice runs out and then it is put at the back of the queue
Preemptive by def (Because it gets preemptive if done with time slice)
	If you increase quantum to inf, becomes FCFS (essentially FCFS with preemption)
Advantages:
	Simple, no starvation
Disadvantages
	Likely a large overhead because context switch
	Reducing the quantum will mean more overhead because of context switches
	IO bound process will run slower on memory loaded system

Essentially the multitasking OS

Performance depends on length of time quantum
	Large time quantum = FCFS like behavior 
	small time quantum = large context switch overhead
Time quantum is usually from 10-100ms
Context switch time is usually 10microseconds
RR has bigger wait time, but better response time for interactive systems
Turnaround time depends on size of time quantum
![[Pasted image 20241101143453.png]]

### Lottery scheduling
Address fairness problem
Process is given some number of tickets based on priority or other properties
OS knows how many tickets have been allocated and one is chosen as random. The process with that ticket is ran for its *time quantum* (not whole time)
	To replicate SJF, shorter jobs get more tickets

To avoid starvation, each process gets at least one ticket

![[Pasted image 20241101144009.png]]
Example 1: There is 1 short job and 1 long job so the ticket diff is 10/1 so 10/11 for short job and 1/11 for long job
Example 2: 0 sj, 1 lj. 2 tix total. Both long jobs have 1 ticket each so 50% each
Example 3: Same but 20 tix total
Example 4: 10 sj, 1lj so 101 tickets total. Each short job has 10/101 chance to run while each long job has 1/101
Example 5: 1 sj and 10lj so 20 tix total. The short job is 10/20 while each long job is 1/20

### Multilevel queue
Multiple queues: foreground and background with them having different scheduling algorithms
	Foreground queue is for interactive
	Background queue is for FCFS
2 scheduling algorithms, one to pick the algorithm and then the algorithm runs
Queues themselves can have priorities and the processes within the queue can have priority
![[Pasted image 20241104140426.png]]

Scheduling done between the queues:
	Fixed priority scheduling: Select the higher priority process in the higher priority queue
		Starvation possible
	Time slice: Each queue gets certain amount of time on CPU to schedule its processes
		aka 80% foreground in RR and 20% background in FCFS


### Multilevel feedback queue scheduling
Processes/jobs can move between the queues based on its features
Example:
	Multiple queues with different priorities
	Round robin scheduling at each priority level
	Highest priority queue first -> next highest -> etc
	Jobs start in highest queue, if time slice expires, move it down, if it does not, then move it up
	This allows the shortest bursts to finish before the highest 
![[Pasted image 20241104141317.png]]
A will run for 1 time and then priority lowered to 1
Then B will run for 1 and then lowered to 1, same with C
Since there are no more jobs in P0, we move to p1
![[Pasted image 20241104141408.png]]
A now runs for 1 and is done (lets say it waits for IO)
Then B runs and finishes
![[Pasted image 20241104141541.png]]
Then lets say A comes back with time slice of 1, since A did not use its last time slice, it goes back to P0 and then runs first for 1 time unit.
Then C runs for 2 out of 3 but then gets moved to P2. 
C then runs in P2 where it finishes (will be moved back to P1 if returns)
![[Pasted image 20241104142007.png]] time = 10

### Solaris dispatch
![[Pasted image 20241104142217.png]]
59 is highest, 0 is lowest. 
Time quantum expired is where it is placed if time quantum is done
return from sleep is where it is placed after being done

Approximating Shortest remaining time first (SRTF) because the CPU bound jobs are lower priority while IO bound will be higher
Unfair for long running jobs
	Add aging: Longer the waiting process, it gets moved up

### Thread scheduling
Contention scope: For user level threads (when mapped to one+ kernel threads)
	PTHREAD_SCOPE_Process: Contend for time on kernel thread between user threads
	PTHREAD_SCOPE_system: Assigned to kernel thread, contends with other kernel threads
Lets say 4 user threads to 1 kernel thread and 4 user thread to 4 kernel thread
So system sees 5 threads total. Each kernel gets 20% (system contention) and each user gets 25% on the kernel (process contention)

When can also tell the thread to inherent the scheduling algo from parent thread or explicitly specify using attribute obj that we have
	SCHED_RR (Round robin), SCHED_FIFO (fifo), and SCHED_OTHER (other)
	Can also set param using schedparam
